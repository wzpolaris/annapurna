{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d12382e",
   "metadata": {},
   "source": [
    "# Notebook 6 – Hamilton Lane Private Assets Fund (HLPAF) Analysis\n",
    "\n",
    "This notebook applies the unified structural model to Hamilton Lane Private Assets Fund,\n",
    "using:\n",
    "- reported Class R monthly returns from the fact sheet,\n",
    "- portfolio composition (strategy, sector, geography, investment type),\n",
    "- the SC/CS/INNOV/TAIL factor set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.factors import (\n",
    "    download_prices,\n",
    "    to_returns,\n",
    "    build_sc_factor,\n",
    "    build_spread_factor,\n",
    "    build_innovation_factor,\n",
    "    build_tail_factor,\n",
    ")\n",
    "\n",
    "tickers = [\"IWM\", \"HYG\", \"IEF\", \"QQQ\", \"^VIX\"]\n",
    "prices = download_prices(tickers, start=\"2005-01-01\")\n",
    "prices.tail()\n",
    "rets = to_returns(prices, freq=\"M\")\n",
    "rets.tail()\n",
    "sc = build_sc_factor(rets, sc_ticker=\"IWM\")\n",
    "cs = build_spread_factor(rets[\"HYG\"], rets[\"IEF\"])\n",
    "innov = build_innovation_factor(rets[\"QQQ\"], sc)\n",
    "\n",
    "# For TAIL, first build ΔVIX from monthly VIX levels\n",
    "vix = prices[\"^VIX\"].resample(\"M\").last().pct_change() * 100.0\n",
    "tail = build_tail_factor(vix)\n",
    "\n",
    "# NEGATE TAIL so that positive beta = exposure to downside risk\n",
    "tail = -tail\n",
    "\n",
    "factors_real = pd.concat([sc, cs, innov, tail], axis=1).dropna()\n",
    "factors_real.columns = [\"SC\", \"CS\", \"INNOV\", \"TAIL\"]\n",
    "print(factors_real.head())\n",
    "print(factors_real.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.unified_model import get_structural_betas\n",
    "from src.overlays import strategy_mix_overlay, sector_overlay, geography_overlay, investment_type_overlay\n",
    "from src.monte_carlo import simulate_private_paths\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851bceba",
   "metadata": {},
   "source": [
    "## 1. Enter HLPAF Class R Monthly Returns (from Fact Sheet)\n",
    "Returns are in percent; we convert to decimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually entered from Hamilton Lane Private Assets Fund fact sheet (Class R):\n",
    "# Years: 2020-09 through 2025-08\n",
    "data_R = {\n",
    "    2020: [None, None, None, None, None, None, None, None, 0.11, 3.72, 1.52, 2.20],\n",
    "    2021: [0.00, 0.27, 2.25, 1.27, 1.69, 4.82, 0.11, 2.67, 0.08, 5.12, -0.03, 2.07],\n",
    "    2022: [-1.68, 0.33, 1.55, -1.36, 2.28, -0.74, 4.74, 1.60, -0.80, 3.94, 3.49, 1.14],\n",
    "    2023: [2.47, 0.06, 1.19, -1.54, 0.14, 2.89, 0.54, -0.85, 1.07, 0.01, 2.19, 2.98],\n",
    "    2024: [0.60, 0.90, 0.51, -0.96, 0.88, 2.18, 0.28, 1.38, 1.76, -0.53, 2.26, -0.40],\n",
    "    2025: [1.58, 0.76, 1.24, 1.29, 2.57, 2.53, 0.31, -0.73, None, None, None, None],\n",
    "}\n",
    "# data_I = {\n",
    "#     2020: [None, None, None, None, None, None, None, None, 0.11, 3.72, 1.52, 2.20],\n",
    "#     2021: [0.00, 0.27, 2.25, 1.28, 1.71, 4.92, 0.17, 2.73, 0.14, 5.18, 0.09, 2.19],\n",
    "#     2022: [-1.56, 0.44, 1.67, -1.30, 2.34, -0.68, 4.80, 1.66, -0.74, 4.00, 3.56, 1.20],\n",
    "#     2023: [2.53, 0.12, 1.25, -1.49, 0.20, 2.95, 0.60, -0.79, 1.13, 0.06, 2.25, 3.04],\n",
    "#     2024: [0.66, 0.96, 0.57, -0.90, 0.94, 2.24, 0.33, 1.44, 1.82, -0.53, 2.37, -0.34],\n",
    "#     2025: [1.64, 0.82, 1.29, 1.35, 2.63, 2.59, 0.37, -0.67, None, None, None, None],\n",
    "# }\n",
    "\n",
    "rows = []\n",
    "for year, vals in data_R.items():\n",
    "    for m, v in enumerate(vals, start=1):\n",
    "        if v is None:\n",
    "            continue\n",
    "        rows.append({\"date\": pd.Timestamp(year=year, month=m, day=1) + pd.offsets.MonthEnd(0), \"ret_pct\": v})\n",
    "df_R = pd.DataFrame(rows).sort_values(\"date\").set_index(\"date\")\n",
    "df_R[\"ret\"] = df_R[\"ret_pct\"] / 100.0\n",
    "df_R.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3d02a",
   "metadata": {},
   "source": [
    "### Basic Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ed119",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df_R['ret']\n",
    "mu = r.mean()\n",
    "sigma = r.std()\n",
    "sigma_ann = sigma * np.sqrt(12)\n",
    "\n",
    "cum = (1 + r).cumprod()\n",
    "roll_max = cum.cummax()\n",
    "dd = cum / roll_max - 1\n",
    "max_dd = dd.min()\n",
    "\n",
    "print('Monthly mean:', f'{mu:.4%}')\n",
    "print('Monthly vol:', f'{sigma:.4%}')\n",
    "print('Annualized vol:', f'{sigma_ann:.4%}')\n",
    "print('Max drawdown:', f'{max_dd:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c99f6",
   "metadata": {},
   "source": [
    "### Plot Monthly Returns and Cumulative NAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e279bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8,6), sharex=False)\n",
    "\n",
    "cum.plot(ax=ax[0])\n",
    "ax[0].set_title('HLPAF Class I - Cumulative Growth Index (since inception)')\n",
    "ax[0].set_ylabel('Cumulative Growth')\n",
    "ax[0].grid(alpha=0.3)\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "ax[1].bar(range(len(r)), r.values, color='steelblue')\n",
    "ax[1].axhline(0, color='black', linewidth=0.8)\n",
    "ax[1].set_title('HLPAF Class I Monthly Returns')\n",
    "ax[1].set_ylabel('Return')\n",
    "ax[1].set_xlabel('')\n",
    "ax[1].set_xlim(-0.5, 59.5) \n",
    "ax[1].grid(axis='y', alpha=0.3)\n",
    "ax[1].tick_params(axis='x', labelbottom=False)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('../artifacts/hlpaf_returns_and_growth.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90637f3",
   "metadata": {},
   "source": [
    "## 2. Desmoothing via AR(1)\n",
    "Estimate AR(1) via proper OLS regression and construct an \"unsmoothed\" return series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bfb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare AR(1) regression: r[t] = c + phi * r[t-1] + epsilon[t]\n",
    "r_lag = r.shift(1).dropna()\n",
    "r_current = r[r_lag.index]\n",
    "\n",
    "# Add constant for OLS\n",
    "X_ar = sm.add_constant(r_lag)\n",
    "y_ar = r_current\n",
    "\n",
    "# Run OLS regression\n",
    "ar1_model = sm.OLS(y_ar, X_ar).fit()\n",
    "\n",
    "# Extract coefficients\n",
    "c = ar1_model.params['const']\n",
    "phi = ar1_model.params['ret']\n",
    "\n",
    "print(\"AR(1) OLS Regression Results:\")\n",
    "print(f\"  Intercept (c): {c:.6f}\")\n",
    "print(f\"  AR(1) coefficient (phi): {phi:.4f}\")\n",
    "print(f\"  t-statistic (phi): {ar1_model.tvalues['ret']:.4f}\")\n",
    "print(f\"  p-value (phi): {ar1_model.pvalues['ret']:.4f}\")\n",
    "print(f\"  R-squared: {ar1_model.rsquared:.4f}\")\n",
    "print()\n",
    "\n",
    "# Print full regression summary\n",
    "print(\"=\"*80)\n",
    "print(\"Full OLS Regression Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(ar1_model.summary())\n",
    "print()\n",
    "\n",
    "# Desmooth using the regression coefficients\n",
    "# r_unsm = (r - phi * r_lag - c) / (1 - phi) + c/(1-phi)\n",
    "# Simplifies to: r_unsm = (r - phi * r_lag) / (1 - phi)\n",
    "r_lag_full = r.shift(1)\n",
    "r_unsm = (r - phi * r_lag_full) / (1 - phi)\n",
    "r_unsm = r_unsm.dropna()\n",
    "\n",
    "mu_unsm = r_unsm.mean()\n",
    "sigma_unsm = r_unsm.std()\n",
    "sigma_unsm_ann = sigma_unsm * np.sqrt(12)\n",
    "\n",
    "print('Desmoothed Statistics:')\n",
    "print(f'  Monthly mean: {mu_unsm:.4%}')\n",
    "print(f'  Monthly vol: {sigma_unsm:.4%}')\n",
    "print(f'  Annualized vol: {sigma_unsm_ann:.4%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451fb5c",
   "metadata": {},
   "source": [
    "### Compare original vs desmoothed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7012df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(r.dropna(), bins=20, alpha=0.6, density=True, label='Original')\n",
    "plt.hist(r_unsm.dropna(), bins=20, alpha=0.6, density=True, label='Desmoothed')\n",
    "plt.legend()\n",
    "plt.title('HLPAF Monthly Returns – Original vs Desmoothed (AR(1))')\n",
    "plt.xlabel('Monthly return')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('../artifacts/hlpaf_desmoothed_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdbf63",
   "metadata": {},
   "source": [
    "## 3. Build HLPAF Structural Betas via Overlays\n",
    "Use unified structural betas and overlay functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural betas for pure Buyout and pure VC\n",
    "betas_bo = get_structural_betas('buyout')\n",
    "betas_vc = get_structural_betas('vc')\n",
    "print('Buyout betas:\\n', betas_bo)\n",
    "print('\\nVC betas:\\n', betas_vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a9641",
   "metadata": {},
   "source": [
    "### 3.1 Strategy Mix Overlay (Buyout / VC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map HLPAF strategy weights into Buyout vs VC weights\n",
    "# Buyout 79%, Growth 11%, Venture 7%, Credit 3%.\n",
    "# Treat Growth as 70% Buyout / 30% VC, Credit as 80% Buyout / 20% VC.\n",
    "w_bo = 0.79 + 0.11*0.7 + 0.03*0.8\n",
    "w_vc = 0.07 + 0.11*0.3 + 0.03*0.2\n",
    "print('Effective Buyout weight:', w_bo)\n",
    "print('Effective VC weight:', w_vc)\n",
    "\n",
    "# Calculate target volatility as weighted average\n",
    "# Buyout: 28% annual = 28%/√12 = 8.08% monthly\n",
    "# VC: 40% annual = 40%/√12 = 11.55% monthly\n",
    "vol_bo_monthly = 0.28 / np.sqrt(12)\n",
    "vol_vc_monthly = 0.40 / np.sqrt(12)\n",
    "target_vol_weighted = w_bo * vol_bo_monthly + w_vc * vol_vc_monthly\n",
    "target_vol_ann_weighted = target_vol_weighted * np.sqrt(12)\n",
    "\n",
    "print(f'\\nTarget volatility:')\n",
    "print(f'  Buyout: {vol_bo_monthly:.2%} monthly ({0.28:.0%} annual)')\n",
    "print(f'  VC: {vol_vc_monthly:.2%} monthly ({0.40:.0%} annual)')\n",
    "print(f'  Weighted: {target_vol_weighted:.2%} monthly ({target_vol_ann_weighted:.2%} annual)')\n",
    "\n",
    "betas_mix = strategy_mix_overlay(betas_bo, betas_vc, w_buyout=w_bo, w_vc=w_vc)\n",
    "print('\\nStrategy-mix betas (HLPAF base):\\n', betas_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d32d10",
   "metadata": {},
   "source": [
    "### 3.2 Sector Overlay (29% Tech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69482911",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_weight = 0.29\n",
    "betas_sector = sector_overlay(betas_mix, tech_weight=tech_weight)\n",
    "print('After sector (tech) overlay:\\n', betas_sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0176a8d",
   "metadata": {},
   "source": [
    "### 3.3 Geography Overlay (71% NA, 22% EU, 3% APAC, 4% ROW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_geo = geography_overlay(betas_sector,\n",
    "                              na_weight=0.71,\n",
    "                              eu_weight=0.22,\n",
    "                              apac_weight=0.03,\n",
    "                              row_weight=0.04)\n",
    "print('After geography overlay:\\n', betas_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b101d5",
   "metadata": {},
   "source": [
    "### 3.4 Investment Type Overlay (co-invest & GP-led concentration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7378e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-invest + single-asset + structured ≈ 49% + 12% + 12% = 73%\n",
    "# Multi-asset secondary + diversified LP-led ≈ 11% + 16% = 27%\n",
    "# Use a high concentration level, e.g. 0.7 on [0,1].\n",
    "betas_hl = investment_type_overlay(betas_geo, concentration_level=0.7)\n",
    "print('Final HLPAF structural betas:\\n', betas_hl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fae77a",
   "metadata": {},
   "source": [
    "## 4. Regress HLPAF Desmoothed Returns on Factors (Optional)\n",
    "This assumes you have built real factors (SC, CS, INNOV, TAIL) as `factors_real`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5488ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Example: align factors_real (from Notebook 3) with r_unsm\n",
    "# factors_real should have columns ['SC','CS','INNOV','TAIL'] at monthly frequency.\n",
    "\n",
    "# from some_path import factors_real  # user: load your factor DataFrame here\n",
    "# For illustration, we create a placeholder with NaNs:\n",
    "#factors_real = pd.DataFrame(index=r_unsm.index, columns=['SC','CS','INNOV','TAIL'])\n",
    "\n",
    "# Replace the above with your real factors before running the regression.\n",
    "df_reg = pd.concat([r_unsm.rename('HLPAF'), factors_real], axis=1).dropna()\n",
    "if not df_reg.empty:\n",
    "    X = sm.add_constant(df_reg[['SC','CS','INNOV','TAIL']])\n",
    "    y = df_reg['HLPAF']\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    print(model.summary())\n",
    "else:\n",
    "    print('No overlapping data between HLPAF returns and factors_real; load real factor data to run regression.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1134c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21485249",
   "metadata": {},
   "source": [
    "## 5. Compare HLPAF vs S&P 500 in Factor Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916115e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Download S&P 500 proxy (SPY) and build monthly log-returns\n",
    "#spy = yf.download('SPY', start='2005-01-01', auto_adjust=True, progress=False)['Adj Close']\n",
    "spy = yf.download('SPY', start='2005-01-01', auto_adjust=True, progress=False)['Close']\n",
    "spy_m = spy.resample('ME').last().pct_change().dropna()\n",
    "spy_m.name = 'SPY'\n",
    "\n",
    "# Align SPY with factors_real\n",
    "df_spy = pd.concat([spy_m, factors_real], axis=1).dropna()\n",
    "if not df_spy.empty:\n",
    "    X_sp = sm.add_constant(df_spy[['SC','CS','INNOV','TAIL']])\n",
    "    y_sp = df_spy['SPY']\n",
    "    model_sp = sm.OLS(y_sp, X_sp).fit()\n",
    "    print(model_sp.summary())\n",
    "    betas_sp = model_sp.params[['SC','CS','INNOV','TAIL']]\n",
    "    print('\\nS&P 500 factor betas (SPY):\\n', betas_sp)\n",
    "else:\n",
    "    print('No overlapping data between SPY and factors_real; load real factor data first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e412f",
   "metadata": {},
   "source": [
    "## 6. Monte Carlo Comparison – HLPAF vs S&P 500\n",
    "Using the estimated/structural betas, simulate distributions and compare VaR/CVaR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c243cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the factors_real data and alignment\n",
    "print(\"Factors shape:\", factors_real.shape)\n",
    "print(\"\\nFactors sample:\")\n",
    "print(factors_real.head())\n",
    "print(\"\\nFactors stats:\")\n",
    "print(factors_real.describe())\n",
    "print(\"\\nBetas HLPAF:\")\n",
    "print(betas_hl)\n",
    "print(\"\\nBetas S&P:\")\n",
    "print(betas_sp)\n",
    "print(\"\\nFACTOR_ORDER:\")\n",
    "from src.unified_model import FACTOR_ORDER\n",
    "print(FACTOR_ORDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.monte_carlo import simulate_factors_normal\n",
    "\n",
    "# Here we assume factors_real is filled with real data\n",
    "if factors_real.dropna().shape[0] > 24:\n",
    "    factors_hist = factors_real.dropna()\n",
    "    betas_sp_reg = betas_sp  # from S&P regression\n",
    "\n",
    "    # Use structural betas for HLPAF\n",
    "    # Strategy: First build the full structural betas with all overlays,\n",
    "    # then calibrate the FINAL result to match the weighted target volatility\n",
    "    \n",
    "    # Target volatility (already calculated from portfolio composition)\n",
    "    target_vol = target_vol_weighted  # 29.31% annual\n",
    "    target_vol_monthly = target_vol\n",
    "    \n",
    "    # The betas_hl already have all overlays applied (from earlier cells)\n",
    "    # Now calibrate these final betas to match the target volatility\n",
    "    factor_cov = factors_hist.cov()\n",
    "    \n",
    "    # Calculate implied volatility from the fully-overlaid structural betas\n",
    "    implied_var = betas_hl @ factor_cov @ betas_hl\n",
    "    implied_vol = np.sqrt(implied_var)\n",
    "    \n",
    "    # Scale to match target\n",
    "    scale_factor = target_vol_monthly / implied_vol\n",
    "    betas_hl_calibrated = betas_hl * scale_factor\n",
    "    \n",
    "    # Verify final volatility\n",
    "    final_vol_hl = np.sqrt(betas_hl_calibrated @ factor_cov @ betas_hl_calibrated)\n",
    "    final_vol_hl_annual = final_vol_hl * np.sqrt(12)\n",
    "    \n",
    "    print(f\"Calibration to target volatility:\")\n",
    "    print(f\"  Target: {target_vol_monthly:.2%} monthly ({target_vol_ann_weighted:.2%} annual)\")\n",
    "    print(f\"  Structural betas implied: {implied_vol:.2%} monthly\")\n",
    "    print(f\"  Scale factor: {scale_factor:.4f}\")\n",
    "    print(f\"  Final (after scaling): {final_vol_hl:.2%} monthly ({final_vol_hl_annual:.2%} annual)\")\n",
    "    \n",
    "    print(f\"\\nOriginal structural betas (all overlays applied):\\n{betas_hl}\")\n",
    "    print(f\"\\nCalibrated structural betas:\\n{betas_hl_calibrated}\")\n",
    "\n",
    "    # Set expected returns based on long-term assumptions\n",
    "    # For HLPAF: 12% annual private equity return\n",
    "    expected_hl_annual = 0.12\n",
    "    expected_hl = (1 + expected_hl_annual)**(1/12) - 1  # 0.949% monthly\n",
    "    \n",
    "    # For S&P 500: 10% annual equity return\n",
    "    expected_sp_annual = 0.10\n",
    "    expected_sp = (1 + expected_sp_annual)**(1/12) - 1  # 0.797% monthly\n",
    "    \n",
    "    print(f\"\\nExpected returns (forward-looking assumptions):\")\n",
    "    print(f\"  HLPAF:   {expected_hl:.2%} monthly ({expected_hl_annual:.1%} annual)\")\n",
    "    print(f\"  S&P 500: {expected_sp:.2%} monthly ({expected_sp_annual:.1%} annual)\")\n",
    "\n",
    "    # Calculate idiosyncratic volatility (should be zero after perfect calibration)\n",
    "    factor_vol_hl = final_vol_hl\n",
    "    idio_vol_hl = np.sqrt(max(0, target_vol_monthly**2 - factor_vol_hl**2))\n",
    "    \n",
    "    # For S&P 500: match historical volatility\n",
    "    # Historical SPY volatility (from earlier cell)\n",
    "    target_sp_vol_monthly = spy_m['SPY'].std()\n",
    "    \n",
    "    # Factor contribution from centered simulation\n",
    "    factor_var_sp = betas_sp_reg.reindex(FACTOR_ORDER).values @ factor_cov.values @ betas_sp_reg.reindex(FACTOR_ORDER).values\n",
    "    factor_vol_sp = np.sqrt(factor_var_sp)\n",
    "    \n",
    "    # Idiosyncratic volatility to match total historical vol\n",
    "    idio_vol_sp = np.sqrt(max(0, target_sp_vol_monthly**2 - factor_vol_sp**2))\n",
    "    \n",
    "    print(f\"\\nVolatility decomposition:\")\n",
    "    print(f\"  HLPAF:   total={target_vol_monthly:.2%}, factor={factor_vol_hl:.2%}, idiosyncratic={idio_vol_hl:.2%}\")\n",
    "    print(f\"  S&P 500: target={target_sp_vol_monthly:.2%}, factor={factor_vol_sp:.2%}, idiosyncratic={idio_vol_sp:.2%} monthly\")\n",
    "\n",
    "    # Simulate 1-year returns directly (12 monthly steps)\n",
    "    rng = np.random.default_rng(123)\n",
    "    n_paths = 10000\n",
    "    n_steps = 12  # 1 year = 12 months\n",
    "    \n",
    "    # For S&P 500: Use HISTORICAL FACTOR MEANS to preserve full volatility\n",
    "    # For HLPAF: Use ZERO-MEAN factors (we add expected return manually)\n",
    "    factor_means = factors_hist.mean()  # Historical means\n",
    "    factor_means_zero = pd.Series(0.0, index=factors_hist.columns)\n",
    "    \n",
    "    # Simulate factors with HISTORICAL means (for S&P 500 volatility)\n",
    "    sims_F = rng.multivariate_normal(\n",
    "        mean=factor_means.values,\n",
    "        cov=factor_cov.values,\n",
    "        size=(n_paths, n_steps)\n",
    "    )\n",
    "    \n",
    "    # Also simulate with ZERO means for comparison\n",
    "    sims_F_zero = rng.multivariate_normal(\n",
    "        mean=factor_means_zero.values,\n",
    "        cov=factor_cov.values,\n",
    "        size=(n_paths, n_steps)\n",
    "    )\n",
    "\n",
    "    from src.unified_model import FACTOR_ORDER\n",
    "\n",
    "    # Map factors to returns for HLPAF and SPY\n",
    "    k = len(FACTOR_ORDER)\n",
    "    beta_hl_vec = betas_hl_calibrated.reindex(FACTOR_ORDER).values.reshape(k,1)\n",
    "    beta_sp_vec = betas_sp_reg.reindex(FACTOR_ORDER).values.reshape(k,1)\n",
    "\n",
    "    # HLPAF: Use zero-mean factors + manual expected return\n",
    "    factor_returns_hl = sims_F_zero @ beta_hl_vec\n",
    "    \n",
    "    # S&P 500: Use historical-mean factors (for volatility preservation)\n",
    "    # but subtract the factor contribution to mean so we can add our expected return\n",
    "    factor_contribution_sp = factor_means.values @ beta_sp_vec  # Historical avg factor contribution\n",
    "    factor_returns_sp = sims_F @ beta_sp_vec - factor_contribution_sp\n",
    "\n",
    "    # Add idiosyncratic risk - sample residuals for each period\n",
    "    eps_hl = rng.normal(0.0, idio_vol_hl, size=factor_returns_hl.shape)  \n",
    "    eps_sp = rng.normal(0.0, idio_vol_sp, size=factor_returns_sp.shape)\n",
    "\n",
    "    # Generate returns:\n",
    "    # HLPAF: manual expected return + zero-mean factors + idiosyncratic\n",
    "    # S&P 500: manual expected return + centered historical-mean factors + idiosyncratic\n",
    "    r_hl = (expected_hl + factor_returns_hl + eps_hl).squeeze(-1)\n",
    "    r_sp = (expected_sp + factor_returns_sp + eps_sp).squeeze(-1)\n",
    "\n",
    "    print(f\"\\nMonthly returns - HL: mean={r_hl.mean():.2%}, std={r_hl.std():.2%}\")\n",
    "    print(f\"Monthly returns - SP: mean={r_sp.mean():.2%}, std={r_sp.std():.2%}\")\n",
    "\n",
    "    # Calculate annual returns by compounding 12 months\n",
    "    ann_hl = (1 + r_hl).prod(axis=1) - 1\n",
    "    ann_sp = (1 + r_sp).prod(axis=1) - 1\n",
    "\n",
    "    def var_cvar(series, alpha=0.05):\n",
    "        q = np.quantile(series, alpha)\n",
    "        cvar = series[series <= q].mean()\n",
    "        return q, cvar\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ANNUAL RETURNS (1-year horizon)\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for name, series in [('HLPAF', ann_hl), ('S&P 500', ann_sp)]:\n",
    "        print(f'\\n--- {name} ---')\n",
    "        print(f'Mean: {series.mean():.2%}, Median: {np.median(series):.2%}, Std: {series.std():.2%}')\n",
    "        for a in [0.10, 0.05, 0.01]:\n",
    "            v, c = var_cvar(series, a)\n",
    "            print(f'  alpha={a}: VaR={v:.2%}, CVaR={c:.2%}')\n",
    "\n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: HLPAF\n",
    "    axes[0].hist(ann_hl, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[0].axvline(np.median(ann_hl), color='red', linestyle='--', linewidth=2, label=f'Median: {np.median(ann_hl):.1%}')\n",
    "    axes[0].set_xlabel('Annual return')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'HLPAF (Structural Model)\\nMean: {ann_hl.mean():.1%}, Std Dev: {ann_hl.std():.1%}')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: S&P 500\n",
    "    axes[1].hist(ann_sp, bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "    axes[1].axvline(np.median(ann_sp), color='red', linestyle='--', linewidth=2, label=f'Median: {np.median(ann_sp):.1%}')\n",
    "    axes[1].set_xlabel('Annual return')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'S&P 500\\nMean: {ann_sp.mean():.1%}, Std Dev: {ann_sp.std():.1%}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Simulated Annual Returns Distribution (1-year horizon)\\nHLPAF: {w_bo:.1%} Buyout + {w_vc:.1%} VC', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('../artifacts/hlpaf_monte_carlo_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('Need sufficient real factor history in factors_real to run Monte Carlo comparison.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between historical S&P 500 and STRUCTURAL HLPAF returns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS: Structural HLPAF vs Historical S&P 500\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find common dates between SPY and factors_hist\n",
    "common_dates = spy_m.index.intersection(factors_hist.index)\n",
    "print(f\"\\nData alignment:\")\n",
    "print(f\"  Period: {common_dates[0].strftime('%Y-%m')} to {common_dates[-1].strftime('%Y-%m')}\")\n",
    "print(f\"  N observations: {len(common_dates)}\")\n",
    "\n",
    "# Get historical S&P 500 returns\n",
    "spy_hist = spy_m.loc[common_dates].values.flatten()\n",
    "\n",
    "# Generate structural HLPAF returns using calibrated betas on historical factors\n",
    "factors_aligned = factors_hist.loc[common_dates]\n",
    "structural_hl = (factors_aligned @ betas_hl_calibrated).values\n",
    "\n",
    "print(f\"\\nStructural HLPAF returns:\")\n",
    "print(f\"  Mean: {structural_hl.mean():.2%} monthly ({((1 + structural_hl.mean())**12 - 1):.1%} annual)\")\n",
    "print(f\"  Std:  {structural_hl.std():.2%} monthly ({structural_hl.std() * np.sqrt(12):.1%} annual)\")\n",
    "\n",
    "print(f\"\\nHistorical S&P 500 returns:\")\n",
    "print(f\"  Mean: {spy_hist.mean():.2%} monthly ({((1 + spy_hist.mean())**12 - 1):.1%} annual)\")\n",
    "print(f\"  Std:  {spy_hist.std():.2%} monthly ({spy_hist.std() * np.sqrt(12):.1%} annual)\")\n",
    "\n",
    "# Calculate unconditional correlation\n",
    "struct_corr = np.corrcoef(spy_hist, structural_hl)[0, 1]\n",
    "print(f\"\\nUnconditional Correlation (Historical S&P 500 vs Structural HLPAF): {struct_corr:.3f}\")\n",
    "\n",
    "# Calculate conditional correlation when S&P 500 < 0\n",
    "negative_mask = spy_hist < 0\n",
    "spy_negative = spy_hist[negative_mask]\n",
    "hl_negative = structural_hl[negative_mask]\n",
    "n_negative = len(spy_negative)\n",
    "\n",
    "if n_negative > 1:\n",
    "    corr_negative = np.corrcoef(spy_negative, hl_negative)[0, 1]\n",
    "    print(f\"\\nConditional Correlation when S&P 500 < 0:\")\n",
    "    print(f\"  N observations: {n_negative} ({n_negative/len(spy_hist)*100:.1f}% of sample)\")\n",
    "    print(f\"  Correlation: {corr_negative:.3f}\")\n",
    "    print(f\"  S&P 500 mean: {spy_negative.mean():.2%}\")\n",
    "    print(f\"  HLPAF mean: {hl_negative.mean():.2%}\")\n",
    "else:\n",
    "    print(f\"\\nInsufficient negative S&P 500 observations for conditional correlation\")\n",
    "\n",
    "# Calculate conditional correlation when S&P 500 >= 0\n",
    "positive_mask = spy_hist >= 0\n",
    "spy_positive = spy_hist[positive_mask]\n",
    "hl_positive = structural_hl[positive_mask]\n",
    "n_positive = len(spy_positive)\n",
    "\n",
    "if n_positive > 1:\n",
    "    corr_positive = np.corrcoef(spy_positive, hl_positive)[0, 1]\n",
    "    print(f\"\\nConditional Correlation when S&P 500 >= 0:\")\n",
    "    print(f\"  N observations: {n_positive} ({n_positive/len(spy_hist)*100:.1f}% of sample)\")\n",
    "    print(f\"  Correlation: {corr_positive:.3f}\")\n",
    "    print(f\"  S&P 500 mean: {spy_positive.mean():.2%}\")\n",
    "    print(f\"  HLPAF mean: {hl_positive.mean():.2%}\")\n",
    "\n",
    "# Scatter plot with conditional highlighting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(spy_negative, hl_negative, alpha=0.6, s=50, color='red', label=f'S&P<0 (ρ={corr_negative:.3f})')\n",
    "plt.scatter(spy_positive, hl_positive, alpha=0.6, s=50, color='blue', label=f'S&P≥0 (ρ={corr_positive:.3f})')\n",
    "plt.xlabel('Historical S&P 500 Monthly Return')\n",
    "plt.ylabel('Structural HLPAF Monthly Return')\n",
    "plt.title(f'Historical S&P 500 vs Structural HLPAF Returns\\nOverall ρ={struct_corr:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/hlpaf_correlation_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Deep dive: Why is correlation LOWER during downturns?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INVESTIGATING CONDITIONAL CORRELATION PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Theory: \"Correlations go to 1 in a crisis\" - but we see the opposite!\n",
    "# Let's check if this is due to:\n",
    "# 1. TAIL factor behavior (PE has high TAIL beta = downside exposure)\n",
    "# 2. Idiosyncratic PE volatility during down markets\n",
    "# 3. Lagged/smoothed PE response to market shocks\n",
    "\n",
    "# Calculate rolling correlation in different market regimes\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Split into quartiles by S&P 500 return\n",
    "quartiles = np.percentile(spy_hist, [25, 50, 75])\n",
    "q1_mask = spy_hist < quartiles[0]  # Worst 25%\n",
    "q2_mask = (spy_hist >= quartiles[0]) & (spy_hist < quartiles[1])\n",
    "q3_mask = (spy_hist >= quartiles[1]) & (spy_hist < quartiles[2])\n",
    "q4_mask = spy_hist >= quartiles[2]  # Best 25%\n",
    "\n",
    "print(f\"\\nCorrelation by S&P 500 return quartile:\")\n",
    "for i, (mask, label) in enumerate([(q1_mask, 'Q1 (worst)'), \n",
    "                                     (q2_mask, 'Q2'), \n",
    "                                     (q3_mask, 'Q3'), \n",
    "                                     (q4_mask, 'Q4 (best)')]):\n",
    "    if mask.sum() > 2:\n",
    "        corr = np.corrcoef(spy_hist[mask], structural_hl[mask])[0, 1]\n",
    "        print(f\"  {label}: ρ={corr:.3f} (N={mask.sum()}, SPY mean={spy_hist[mask].mean():.2%})\")\n",
    "\n",
    "# Check if TAIL factor explains the pattern\n",
    "print(f\"\\nFactor contribution analysis during down markets:\")\n",
    "factors_aligned_full = factors_hist.loc[common_dates]\n",
    "print(f\"\\nTAIL factor behavior:\")\n",
    "print(f\"  When S&P<0: TAIL mean={factors_aligned_full.loc[common_dates[negative_mask], 'TAIL'].mean():.2%}\")\n",
    "print(f\"  When S&P>=0: TAIL mean={factors_aligned_full.loc[common_dates[positive_mask], 'TAIL'].mean():.2%}\")\n",
    "print(f\"  HLPAF TAIL beta: {betas_hl_calibrated['TAIL']:.3f}\")\n",
    "\n",
    "# Check SC (market) factor\n",
    "print(f\"\\nSC (market) factor behavior:\")\n",
    "print(f\"  When S&P<0: SC mean={factors_aligned_full.loc[common_dates[negative_mask], 'SC'].mean():.2%}\")\n",
    "print(f\"  When S&P>=0: SC mean={factors_aligned_full.loc[common_dates[positive_mask], 'SC'].mean():.2%}\")\n",
    "print(f\"  HLPAF SC beta: {betas_hl_calibrated['SC']:.3f}\")\n",
    "\n",
    "# Calculate explained vs residual variance in each regime\n",
    "print(f\"\\nVariance decomposition:\")\n",
    "for mask, label in [(negative_mask, 'S&P<0'), (positive_mask, 'S&P>=0')]:\n",
    "    if mask.sum() > 2:\n",
    "        hl_regime = structural_hl[mask]\n",
    "        spy_regime = spy_hist[mask]\n",
    "        \n",
    "        # Beta of HLPAF on SPY in this regime\n",
    "        cov = np.cov(spy_regime, hl_regime)[0, 1]\n",
    "        var_spy = np.var(spy_regime)\n",
    "        beta_regime = cov / var_spy if var_spy > 0 else 0\n",
    "        \n",
    "        # R² = correlation²\n",
    "        r_sq = np.corrcoef(spy_regime, hl_regime)[0, 1]**2\n",
    "        \n",
    "        print(f\"\\n  {label}:\")\n",
    "        print(f\"    HLPAF std: {hl_regime.std():.2%}\")\n",
    "        print(f\"    SPY std: {spy_regime.std():.2%}\")\n",
    "        print(f\"    Beta (HLPAF on SPY): {beta_regime:.2f}\")\n",
    "        print(f\"    R²: {r_sq:.1%} (explained variance)\")\n",
    "        print(f\"    Idiosyncratic: {(1-r_sq):.1%} (unexplained)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Lower correlation during downturns could indicate:\")\n",
    "print(\"1. PE has IDIOSYNCRATIC downside risk beyond public markets\")\n",
    "print(\"   (illiquidity, forced sales, operational stress)\")\n",
    "print(\"2. TAIL factor adds PE-specific volatility in crises\")\n",
    "print(\"3. Different companies/sectors drive PE vs public market losses\")\n",
    "print(\"\\nThis is WORSE than 'correlations go to 1' - you get MORE\")\n",
    "print(\"uncorrelated downside risk when markets fall!\")\n",
    "\n",
    "# Quantify downside vs upside beta\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNSIDE BETA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Downside beta: regression when S&P < 0\n",
    "hl_down = structural_hl[negative_mask]\n",
    "spy_down = spy_hist[negative_mask]\n",
    "beta_down = np.cov(spy_down, hl_down)[0,1] / np.var(spy_down)\n",
    "\n",
    "# Upside beta: regression when S&P >= 0  \n",
    "hl_up = structural_hl[positive_mask]\n",
    "spy_up = spy_hist[positive_mask]\n",
    "beta_up = np.cov(spy_up, hl_up)[0,1] / np.var(spy_up)\n",
    "\n",
    "# Overall beta\n",
    "beta_all = np.cov(spy_hist, structural_hl)[0,1] / np.var(spy_hist)\n",
    "\n",
    "print(f\"\\nBeta (HLPAF sensitivity to S&P 500):\")\n",
    "print(f\"  Downside beta (S&P<0):  {beta_down:.2f}\")\n",
    "print(f\"  Upside beta (S&P≥0):    {beta_up:.2f}\")\n",
    "print(f\"  Overall beta:           {beta_all:.2f}\")\n",
    "print(f\"  Ratio (down/up):        {beta_down/beta_up:.2f}x\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  • When S&P drops 1%, HLPAF drops {abs(beta_down):.2f}%\")\n",
    "print(f\"  • When S&P rises 1%, HLPAF rises {beta_up:.2f}%\")\n",
    "print(f\"  • You get {beta_down/beta_up:.1f}x MORE downside than upside!\")\n",
    "\n",
    "# Calculate downside capture vs upside capture\n",
    "down_capture = (hl_negative.mean() / spy_negative.mean()) * 100\n",
    "up_capture = (hl_positive.mean() / spy_positive.mean()) * 100\n",
    "\n",
    "print(f\"\\nCapture ratios (mean returns):\")\n",
    "print(f\"  Downside capture: {down_capture:.0f}%\")\n",
    "print(f\"  Upside capture:   {up_capture:.0f}%\")\n",
    "\n",
    "print(f\"\\nWhen S&P is negative (averaging -3.66%):\")\n",
    "print(f\"  → HLPAF averages -13.82% (captures {down_capture:.0f}% of downside)\")\n",
    "print(f\"\\nWhen S&P is positive (averaging +3.36%):\")  \n",
    "print(f\"  → HLPAF averages -1.09% (captures {up_capture:.0f}% of upside)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ROOT CAUSE: High TAIL Beta + Operating Leverage\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TAIL beta = {betas_hl_calibrated['TAIL']:.3f}\")\n",
    "print(f\"\\nIn down markets, TAIL factor averages -58.47%\")\n",
    "print(f\"This adds {betas_hl_calibrated['TAIL'] * -0.5847:.1%} to HLPAF losses\")\n",
    "print(f\"\\nCombined with market beta (SC={betas_hl_calibrated['SC']:.3f}),\")\n",
    "print(f\"PE gets hit TWICE: market decline + credit/liquidity stress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80785493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reality check: Is the TAIL beta realistic?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TAIL BETA REALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check base structural betas before overlays and calibration\n",
    "print(f\"\\nBase structural betas (from unified_model.py):\")\n",
    "print(f\"  Buyout TAIL beta: {betas_bo['TAIL']:.2f}\")\n",
    "print(f\"  VC TAIL beta:     {betas_vc['TAIL']:.2f}\")\n",
    "\n",
    "# After strategy mix\n",
    "print(f\"\\nAfter strategy mix ({w_bo:.1%} Buyout, {w_vc:.1%} VC):\")\n",
    "print(f\"  Mixed TAIL beta:  {betas_mix['TAIL']:.3f}\")\n",
    "\n",
    "# After all overlays (before calibration)\n",
    "print(f\"\\nAfter all overlays (sector, geo, investment type):\")\n",
    "print(f\"  Final TAIL beta (before vol calibration): {betas_hl['TAIL']:.3f}\")\n",
    "\n",
    "# After volatility calibration\n",
    "print(f\"\\nAfter volatility calibration (scaling by {scale_factor:.4f}):\")\n",
    "print(f\"  Calibrated TAIL beta: {betas_hl_calibrated['TAIL']:.3f}\")\n",
    "\n",
    "# Show the impact\n",
    "tail_factor_down = -0.5847  # TAIL factor mean when S&P < 0\n",
    "tail_contribution = betas_hl_calibrated['TAIL'] * tail_factor_down\n",
    "\n",
    "print(f\"\\nTAIL factor impact in down markets:\")\n",
    "print(f\"  TAIL factor = {tail_factor_down:.2%}\")\n",
    "print(f\"  TAIL beta × factor = {betas_hl_calibrated['TAIL']:.3f} × {tail_factor_down:.2%}\")\n",
    "print(f\"  = {tail_contribution:.2%} contribution to monthly return\")\n",
    "\n",
    "# Compare to SC (market) contribution\n",
    "sc_factor_down = -0.0500  # SC factor mean when S&P < 0\n",
    "sc_contribution = betas_hl_calibrated['SC'] * sc_factor_down\n",
    "\n",
    "print(f\"\\nSC (market) factor impact in down markets:\")\n",
    "print(f\"  SC factor = {sc_factor_down:.2%}\")\n",
    "print(f\"  SC beta × factor = {betas_hl_calibrated['SC']:.3f} × {sc_factor_down:.2%}\")\n",
    "print(f\"  = {sc_contribution:.2%} contribution to monthly return\")\n",
    "\n",
    "print(f\"\\nTotal factor contribution in down markets:\")\n",
    "print(f\"  SC:   {sc_contribution:.2%}\")\n",
    "print(f\"  TAIL: {tail_contribution:.2%}\")\n",
    "print(f\"  Other factors: ~{-0.1382 - sc_contribution - tail_contribution:.2%}\")\n",
    "print(f\"  Total structural HLPAF: -13.82%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"The TAIL beta of {betas_hl_calibrated['TAIL']:.3f} seems reasonable because:\")\n",
    "print(f\"1. Base structural assumption: Buyout=1.2, VC=2.0\")\n",
    "print(f\"2. HLPAF is {w_bo:.1%} Buyout (lower leverage) + {w_vc:.1%} VC\")\n",
    "print(f\"3. Overlays reduce it further (diversification effects)\")\n",
    "print(f\"4. BUT: TAIL factor itself is HUGE in crises (-58% vs -5% for SC)\")\n",
    "print(f\"5. So even 'modest' TAIL beta of 0.225 creates {tail_contribution:.1%} losses\")\n",
    "print(f\"\\nThe issue isn't the beta - it's that VIX spikes 8-10x more\")\n",
    "print(f\"than the market drops in crises, creating amplified PE losses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d146cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check S&P 500 volatility calculation\n",
    "print(\"=\"*60)\n",
    "print(\"S&P 500 VOLATILITY DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMonthly S&P 500 returns:\")\n",
    "print(f\"  Mean: {r_sp.mean():.2%}\")\n",
    "print(f\"  Std: {r_sp.std():.2%}\")\n",
    "print(f\"  Annualized (std × √12): {r_sp.std() * np.sqrt(12):.2%}\")\n",
    "\n",
    "print(f\"\\nAnnual S&P 500 returns (compounded):\")\n",
    "print(f\"  Mean: {ann_sp.mean():.2%}\")\n",
    "print(f\"  Std: {ann_sp.std():.2%}\")\n",
    "\n",
    "print(f\"\\nHistorical SPY (full sample):\")\n",
    "print(f\"  Monthly mean: {spy_m['SPY'].mean():.2%}\")\n",
    "print(f\"  Monthly std: {spy_m['SPY'].std():.2%}\")\n",
    "print(f\"  Annualized vol: {spy_m['SPY'].std() * np.sqrt(12):.2%}\")\n",
    "\n",
    "print(f\"\\nSolution: Adjusted idiosyncratic volatility to match historical total:\")\n",
    "print(f\"  Factor vol: 3.02% monthly (from β'Σβ)\")\n",
    "print(f\"  Idio vol:   3.04% monthly (adjusted)\")\n",
    "print(f\"  Total:      4.28% monthly ≈ 4.29% historical ✓\")\n",
    "print(f\"  R² from regression: {model_sp.rsquared:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd185d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correct S&P 500 idiosyncratic volatility to match historical total vol\n",
    "# Historical: 4.29% monthly total vol\n",
    "# Factor contribution from centered simulation: need to calculate\n",
    "\n",
    "# Target: match historical SPY volatility\n",
    "target_sp_vol_monthly = spy_m['SPY'].std()\n",
    "\n",
    "# Factor volatility from regression (using centered factors)\n",
    "# This is what we get from beta' * Cov * beta\n",
    "factor_var_sp = beta_sp_vec.T @ factor_cov.values @ beta_sp_vec\n",
    "factor_vol_sp = np.sqrt(factor_var_sp)[0,0]\n",
    "\n",
    "print(f\"\\nS&P 500 volatility breakdown:\")\n",
    "print(f\"  Target (historical): {target_sp_vol_monthly:.2%} monthly\")\n",
    "print(f\"  Factor contribution: {factor_vol_sp:.2%} monthly\")\n",
    "print(f\"  Current idiosyncratic: {idio_vol_sp:.2%} monthly\")\n",
    "print(f\"  Current total: {np.sqrt(factor_vol_sp**2 + idio_vol_sp**2):.2%} monthly\")\n",
    "\n",
    "# To match target, we need:\n",
    "# target² = factor² + idio²\n",
    "# idio² = target² - factor²\n",
    "idio_vol_sp_corrected = np.sqrt(max(0, target_sp_vol_monthly**2 - factor_vol_sp**2))\n",
    "print(f\"\\n  Corrected idiosyncratic: {idio_vol_sp_corrected:.2%} monthly\")\n",
    "print(f\"  Corrected total: {np.sqrt(factor_vol_sp**2 + idio_vol_sp_corrected**2):.2%} monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b19ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the factor model is contributing to returns\n",
    "print(\"=\"*60)\n",
    "print(\"FACTOR CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nHistorical factor means (monthly):\")\n",
    "print(factors_hist.mean())\n",
    "print(f\"\\nFactor contribution to HLPAF (using calibrated betas):\")\n",
    "factor_contrib_hl = float(betas_hl_calibrated @ factors_hist.mean())\n",
    "print(f\"  {factor_contrib_hl:.2%} monthly = {((1 + factor_contrib_hl)**12 - 1):.1%} annual\")\n",
    "print(f\"\\nThis explains why mean return is so high!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the sign/interpretation of TAIL factor\n",
    "print(\"=\"*60)\n",
    "print(\"TAIL FACTOR DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTAIL factor mean: {factors_hist['TAIL'].mean():.4f}\")\n",
    "print(f\"TAIL factor std: {factors_hist['TAIL'].std():.4f}\")\n",
    "print(f\"\\nTAIL factor should represent DOWNSIDE risk (VIX spikes).\")\n",
    "print(f\"A positive TAIL mean of {factors_hist['TAIL'].mean():.2%} suggests\")\n",
    "print(f\"the sign convention may need to be flipped.\")\n",
    "print(f\"\\nStructural betas on TAIL:\")\n",
    "print(f\"  VC: {betas_vc['TAIL']:.3f}\")\n",
    "print(f\"  Buyout: {betas_bo['TAIL']:.3f}\")\n",
    "print(f\"  HLPAF: {betas_hl['TAIL']:.3f}\")\n",
    "print(f\"\\nIf these are POSITIVE, they imply PE benefits from tail events,\")\n",
    "print(f\"which is backwards. We should NEGATE the TAIL factor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check: what does the factor model imply vs actual S&P 500 returns?\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING S&P 500 RETURN ATTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Historical S&P 500 mean return\n",
    "spy_mean_monthly = float(spy_m.mean())\n",
    "spy_mean_annual = (1 + spy_mean_monthly)**12 - 1\n",
    "\n",
    "print(f\"\\nHistorical S&P 500 (SPY):\")\n",
    "print(f\"  Monthly mean: {spy_mean_monthly:.2%}\")\n",
    "print(f\"  Annual mean: {spy_mean_annual:.2%}\")\n",
    "\n",
    "# Factor model prediction for S&P 500\n",
    "factor_means = factors_hist.mean()\n",
    "predicted_sp_return = float(betas_sp_reg @ factor_means)\n",
    "\n",
    "print(f\"\\nFactor model prediction:\")\n",
    "print(f\"  Factor means:\\n{factor_means}\")\n",
    "print(f\"  S&P 500 betas:\\n{betas_sp_reg}\")\n",
    "print(f\"  Predicted monthly return: {predicted_sp_return:.2%}\")\n",
    "print(f\"  Predicted annual return: {((1 + predicted_sp_return)**12 - 1):.2%}\")\n",
    "\n",
    "print(f\"\\nRegression intercept: {model_sp.params['const']:.2%}\")\n",
    "print(f\"Total predicted (intercept + factors): {(model_sp.params['const'] + predicted_sp_return):.2%} monthly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check R-squared for S&P 500 factor model\n",
    "print(\"=\"*60)\n",
    "print(\"S&P 500 FACTOR MODEL FIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nR-squared: {model_sp.rsquared:.4f} ({model_sp.rsquared*100:.2f}%)\")\n",
    "print(f\"Adjusted R-squared: {model_sp.rsquared_adj:.4f} ({model_sp.rsquared_adj*100:.2f}%)\")\n",
    "print(f\"\\nThis means the 4 factors (SC, CS, INNOV, TAIL) explain\")\n",
    "print(f\"{model_sp.rsquared*100:.1f}% of S&P 500 return variance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6474184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison: Desmoothed HLPAF vs Structural Model vs S&P 500\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON: DESMOOTHED vs STRUCTURAL MODEL vs S&P 500\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not df_reg.empty:\n",
    "    # Calculate structural returns (factor-based)\n",
    "    factor_returns = df_reg[['SC','CS','INNOV','TAIL']]\n",
    "    structural_returns = (factor_returns @ betas_hl_calibrated).values\n",
    "    desmoothed_returns = df_reg['HLPAF'].values\n",
    "    \n",
    "    # Get S&P 500 returns aligned with HLPAF period\n",
    "    common_dates = df_reg.index.intersection(spy_m.index)\n",
    "    spy_aligned = spy_m.loc[common_dates].values.flatten()\n",
    "    \n",
    "    # Need to align all three series to common dates\n",
    "    desmoothed_aligned = df_reg.loc[common_dates, 'HLPAF'].values\n",
    "    factors_aligned = df_reg.loc[common_dates, ['SC','CS','INNOV','TAIL']]\n",
    "    structural_aligned = (factors_aligned @ betas_hl_calibrated).values\n",
    "    \n",
    "    print(f\"\\nAnalysis period: {common_dates[0].strftime('%Y-%m')} to {common_dates[-1].strftime('%Y-%m')}\")\n",
    "    print(f\"Number of observations: {len(common_dates)}\")\n",
    "    \n",
    "    # Calculate statistics for all three series\n",
    "    def calculate_stats(returns, name):\n",
    "        \"\"\"Calculate comprehensive statistics for a return series\"\"\"\n",
    "        stats = {}\n",
    "        stats['name'] = name\n",
    "        stats['mean_monthly'] = returns.mean()\n",
    "        stats['mean_annual'] = (1 + returns.mean())**12 - 1\n",
    "        stats['vol_monthly'] = returns.std()\n",
    "        stats['vol_annual'] = returns.std() * np.sqrt(12)\n",
    "        stats['sharpe_annual'] = stats['mean_annual'] / stats['vol_annual'] if stats['vol_annual'] > 0 else 0\n",
    "        \n",
    "        # Downside statistics (when return < 0)\n",
    "        downside_mask = returns < 0\n",
    "        if downside_mask.sum() > 0:\n",
    "            stats['downside_mean'] = returns[downside_mask].mean()\n",
    "            stats['downside_vol'] = returns[downside_mask].std()\n",
    "            stats['downside_freq'] = downside_mask.sum() / len(returns)\n",
    "        else:\n",
    "            stats['downside_mean'] = 0\n",
    "            stats['downside_vol'] = 0\n",
    "            stats['downside_freq'] = 0\n",
    "        \n",
    "        # VaR and CVaR at 5% level\n",
    "        stats['var_5'] = np.percentile(returns, 5)\n",
    "        cvar_mask = returns <= stats['var_5']\n",
    "        stats['cvar_5'] = returns[cvar_mask].mean() if cvar_mask.sum() > 0 else stats['var_5']\n",
    "        \n",
    "        # Max drawdown (approximate from monthly returns)\n",
    "        cum = (1 + returns).cumprod()\n",
    "        roll_max = np.maximum.accumulate(cum)\n",
    "        dd = cum / roll_max - 1\n",
    "        stats['max_dd'] = dd.min()\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    # Calculate stats for all three series\n",
    "    stats_desm = calculate_stats(desmoothed_aligned, 'Desmoothed HLPAF')\n",
    "    stats_struct = calculate_stats(structural_aligned, 'Structural Model')\n",
    "    stats_spy = calculate_stats(spy_aligned, 'S&P 500')\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_desm_spy = np.corrcoef(desmoothed_aligned, spy_aligned)[0, 1]\n",
    "    corr_struct_spy = np.corrcoef(structural_aligned, spy_aligned)[0, 1]\n",
    "    corr_desm_struct = np.corrcoef(desmoothed_aligned, structural_aligned)[0, 1]\n",
    "    \n",
    "    # Calculate downside correlations (when S&P < 0)\n",
    "    spy_down_mask = spy_aligned < 0\n",
    "    if spy_down_mask.sum() > 1:\n",
    "        corr_desm_spy_down = np.corrcoef(desmoothed_aligned[spy_down_mask], \n",
    "                                         spy_aligned[spy_down_mask])[0, 1]\n",
    "        corr_struct_spy_down = np.corrcoef(structural_aligned[spy_down_mask], \n",
    "                                           spy_aligned[spy_down_mask])[0, 1]\n",
    "    else:\n",
    "        corr_desm_spy_down = np.nan\n",
    "        corr_struct_spy_down = np.nan\n",
    "    \n",
    "    # Calculate betas (on S&P 500)\n",
    "    def calc_beta(returns_y, returns_x):\n",
    "        cov = np.cov(returns_x, returns_y)[0, 1]\n",
    "        var = np.var(returns_x)\n",
    "        return cov / var if var > 0 else 0\n",
    "    \n",
    "    beta_desm_all = calc_beta(desmoothed_aligned, spy_aligned)\n",
    "    beta_struct_all = calc_beta(structural_aligned, spy_aligned)\n",
    "    \n",
    "    # Downside betas\n",
    "    if spy_down_mask.sum() > 1:\n",
    "        beta_desm_down = calc_beta(desmoothed_aligned[spy_down_mask], spy_aligned[spy_down_mask])\n",
    "        beta_struct_down = calc_beta(structural_aligned[spy_down_mask], spy_aligned[spy_down_mask])\n",
    "    else:\n",
    "        beta_desm_down = np.nan\n",
    "        beta_struct_down = np.nan\n",
    "    \n",
    "    # Upside betas (when S&P >= 0)\n",
    "    spy_up_mask = spy_aligned >= 0\n",
    "    if spy_up_mask.sum() > 1:\n",
    "        beta_desm_up = calc_beta(desmoothed_aligned[spy_up_mask], spy_aligned[spy_up_mask])\n",
    "        beta_struct_up = calc_beta(structural_aligned[spy_up_mask], spy_aligned[spy_up_mask])\n",
    "    else:\n",
    "        beta_desm_up = np.nan\n",
    "        beta_struct_up = np.nan\n",
    "    \n",
    "    # Print comprehensive comparison table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Format the table\n",
    "    header = f\"{'Metric':<30} {'Desmoothed':>15} {'Structural':>15} {'S&P 500':>15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Return metrics\n",
    "    print(f\"{'Mean (monthly)':<30} {stats_desm['mean_monthly']:>14.2%} {stats_struct['mean_monthly']:>14.2%} {stats_spy['mean_monthly']:>14.2%}\")\n",
    "    print(f\"{'Mean (annual)':<30} {stats_desm['mean_annual']:>14.2%} {stats_struct['mean_annual']:>14.2%} {stats_spy['mean_annual']:>14.2%}\")\n",
    "    print()\n",
    "    \n",
    "    # Volatility metrics\n",
    "    print(f\"{'Vol (monthly)':<30} {stats_desm['vol_monthly']:>14.2%} {stats_struct['vol_monthly']:>14.2%} {stats_spy['vol_monthly']:>14.2%}\")\n",
    "    print(f\"{'Vol (annual)':<30} {stats_desm['vol_annual']:>14.2%} {stats_struct['vol_annual']:>14.2%} {stats_spy['vol_annual']:>14.2%}\")\n",
    "    print(f\"{'Sharpe Ratio (annual)':<30} {stats_desm['sharpe_annual']:>14.2f} {stats_struct['sharpe_annual']:>14.2f} {stats_spy['sharpe_annual']:>14.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Downside metrics\n",
    "    print(f\"{'Downside freq (%)':<30} {stats_desm['downside_freq']:>14.1%} {stats_struct['downside_freq']:>14.1%} {stats_spy['downside_freq']:>14.1%}\")\n",
    "    print(f\"{'Downside mean':<30} {stats_desm['downside_mean']:>14.2%} {stats_struct['downside_mean']:>14.2%} {stats_spy['downside_mean']:>14.2%}\")\n",
    "    print(f\"{'Downside vol':<30} {stats_desm['downside_vol']:>14.2%} {stats_struct['downside_vol']:>14.2%} {stats_spy['downside_vol']:>14.2%}\")\n",
    "    print()\n",
    "    \n",
    "    # Risk metrics\n",
    "    print(f\"{'VaR (5%)':<30} {stats_desm['var_5']:>14.2%} {stats_struct['var_5']:>14.2%} {stats_spy['var_5']:>14.2%}\")\n",
    "    print(f\"{'CVaR (5%)':<30} {stats_desm['cvar_5']:>14.2%} {stats_struct['cvar_5']:>14.2%} {stats_spy['cvar_5']:>14.2%}\")\n",
    "    print(f\"{'Max Drawdown':<30} {stats_desm['max_dd']:>14.2%} {stats_struct['max_dd']:>14.2%} {stats_spy['max_dd']:>14.2%}\")\n",
    "    \n",
    "    # Correlation table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRELATION WITH S&P 500\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Metric':<30} {'Desmoothed':>15} {'Structural':>15}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Overall correlation':<30} {corr_desm_spy:>14.3f} {corr_struct_spy:>14.3f}\")\n",
    "    print(f\"{'Downside correlation':<30} {corr_desm_spy_down:>14.3f} {corr_struct_spy_down:>14.3f}\")\n",
    "    print()\n",
    "    print(f\"{'Overall beta':<30} {beta_desm_all:>14.2f} {beta_struct_all:>14.2f}\")\n",
    "    print(f\"{'Downside beta (S&P<0)':<30} {beta_desm_down:>14.2f} {beta_struct_down:>14.2f}\")\n",
    "    print(f\"{'Upside beta (S&P≥0)':<30} {beta_desm_up:>14.2f} {beta_struct_up:>14.2f}\")\n",
    "    print(f\"{'Beta ratio (down/up)':<30} {beta_desm_down/beta_desm_up:>14.2f} {beta_struct_down/beta_struct_up:>14.2f}\")\n",
    "    \n",
    "    # Model validation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Correlation (Desmoothed vs Structural): {corr_desm_struct:.3f}\")\n",
    "    tracking_error = np.std(structural_aligned - desmoothed_aligned) * np.sqrt(12)\n",
    "    print(f\"Tracking error: {tracking_error:.2%} (annualized)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"1. Desmoothed HLPAF has {corr_desm_spy:.1%} correlation with S&P 500\")\n",
    "    print(f\"   Structural model has {corr_struct_spy:.1%} correlation with S&P 500\")\n",
    "    print(f\"\\n2. Both show asymmetric downside exposure:\")\n",
    "    print(f\"   Desmoothed: {beta_desm_down/beta_desm_up:.1f}x more downside beta\")\n",
    "    print(f\"   Structural: {beta_struct_down/beta_struct_up:.1f}x more downside beta\")\n",
    "    print(f\"\\n3. Structural model correlation with actual HLPAF: {corr_desm_struct:.1%}\")\n",
    "    print(f\"   (Lower correlation expected as structural betas are calibrated, not fitted)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo data available for comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive: Why near-zero downside beta but large downside losses?\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLAINING THE PARADOX: Low Downside Beta vs Large Downside Losses\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not df_reg.empty:\n",
    "    # Re-align data\n",
    "    common_dates = df_reg.index.intersection(spy_m.index)\n",
    "    spy_aligned = spy_m.loc[common_dates].values.flatten()\n",
    "    desmoothed_aligned = df_reg.loc[common_dates, 'HLPAF'].values\n",
    "    factors_aligned = df_reg.loc[common_dates, ['SC','CS','INNOV','TAIL']]\n",
    "    structural_aligned = (factors_aligned @ betas_hl_calibrated).values\n",
    "    \n",
    "    # Split into downside regime (S&P < 0)\n",
    "    spy_down_mask = spy_aligned < 0\n",
    "    \n",
    "    print(f\"\\nDownside regime analysis (S&P 500 < 0):\")\n",
    "    print(f\"Number of months: {spy_down_mask.sum()} ({spy_down_mask.sum()/len(spy_aligned)*100:.1f}% of sample)\")\n",
    "    \n",
    "    # Factor contributions in downside regime\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FACTOR DECOMPOSITION DURING DOWNSIDE MONTHS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate each factor's contribution\n",
    "    factors_down = factors_aligned[spy_down_mask]\n",
    "    \n",
    "    print(f\"\\nFactor means when S&P 500 < 0:\")\n",
    "    print(f\"{'Factor':<15} {'Mean':<12} {'HLPAF Beta':<12} {'Contribution':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    factor_contributions = {}\n",
    "    for factor in ['SC', 'CS', 'INNOV', 'TAIL']:\n",
    "        mean_factor = factors_down[factor].mean()\n",
    "        beta_factor = betas_hl_calibrated[factor]\n",
    "        contribution = mean_factor * beta_factor\n",
    "        factor_contributions[factor] = contribution\n",
    "        print(f\"{factor:<15} {mean_factor:>11.2%} {beta_factor:>11.3f} {contribution:>11.2%}\")\n",
    "    \n",
    "    total_factor_contrib = sum(factor_contributions.values())\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'TOTAL':<15} {'':<12} {'':<12} {total_factor_contrib:>11.2%}\")\n",
    "    \n",
    "    # Actual structural return in downside regime\n",
    "    actual_structural_down = structural_aligned[spy_down_mask].mean()\n",
    "    print(f\"\\nActual structural HLPAF mean (S&P<0): {actual_structural_down:.2%}\")\n",
    "    print(f\"Factor-based calculation:             {total_factor_contrib:.2%}\")\n",
    "    \n",
    "    # Now explain the beta calculation\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"WHY IS DOWNSIDE BETA NEAR ZERO?\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Beta measures linear relationship: Cov(HLPAF, SPY) / Var(SPY)\n",
    "    # In downside regime only:\n",
    "    spy_down = spy_aligned[spy_down_mask]\n",
    "    hlpaf_down = structural_aligned[spy_down_mask]\n",
    "    \n",
    "    cov_down = np.cov(spy_down, hlpaf_down)[0, 1]\n",
    "    var_spy_down = np.var(spy_down)\n",
    "    beta_down = cov_down / var_spy_down\n",
    "    corr_down = np.corrcoef(spy_down, hlpaf_down)[0, 1]\n",
    "    \n",
    "    print(f\"\\nDownside beta calculation:\")\n",
    "    print(f\"  Cov(HLPAF, S&P | S&P<0) = {cov_down:.6f}\")\n",
    "    print(f\"  Var(S&P | S&P<0)        = {var_spy_down:.6f}\")\n",
    "    print(f\"  Beta = Cov/Var          = {beta_down:.3f}\")\n",
    "    print(f\"  Correlation             = {corr_down:.3f}\")\n",
    "    \n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"Beta ≈ 0 means: 'HLPAF losses do NOT scale linearly with S&P 500 losses'\")\n",
    "    print(f\"           NOT: 'HLPAF has no downside risk'\")\n",
    "    \n",
    "    # Demonstrate with scatter plot analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"THE TAIL FACTOR CREATES NON-LINEAR CRISIS EXPOSURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show that TAIL is not perfectly correlated with S&P in downside\n",
    "    tail_down = factors_down['TAIL'].values\n",
    "    sc_down = factors_down['SC'].values\n",
    "    \n",
    "    corr_tail_spy = np.corrcoef(tail_down, spy_down)[0, 1]\n",
    "    corr_sc_spy = np.corrcoef(sc_down, spy_down)[0, 1]\n",
    "    \n",
    "    print(f\"\\nCorrelations during S&P downside months:\")\n",
    "    print(f\"  SC factor ↔ S&P 500:   {corr_sc_spy:.3f}\")\n",
    "    print(f\"  TAIL factor ↔ S&P 500: {corr_tail_spy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nKey insight:\")\n",
    "    print(f\"  SC (market factor) has {corr_sc_spy:.1%} correlation with S&P → creates LINEAR beta\")\n",
    "    print(f\"  TAIL (VIX) has {corr_tail_spy:.1%} correlation with S&P → creates NON-LINEAR CRISIS EXPOSURE\")\n",
    "    \n",
    "    print(f\"\\nTAIL factor behavior:\")\n",
    "    print(f\"  - When S&P drops 5%, VIX might spike 20% (March 2020)\")\n",
    "    print(f\"  - When S&P drops 5%, VIX might spike 50% (October 2008)\")\n",
    "    print(f\"  - When S&P drops 5%, VIX might only rise 5% (typical correction)\")\n",
    "    print(f\"  → Same S&P move, wildly different TAIL impact!\")\n",
    "    \n",
    "    print(f\"\\nThis creates:\")\n",
    "    print(f\"  1. Large AVERAGE downside losses (mean TAIL = {factors_down['TAIL'].mean():.1%})\")\n",
    "    print(f\"  2. High VARIANCE in those losses (TAIL std = {factors_down['TAIL'].std():.1%})\")\n",
    "    print(f\"  3. Low CORRELATION with S&P (correlation = {corr_tail_spy:.2f})\")\n",
    "    print(f\"  4. Therefore: Low BETA but high DOWNSIDE RISK\")\n",
    "    \n",
    "    # Quantify the risk\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RISK DECOMPOSITION: Linear Beta vs Non-Linear Crisis Exposure\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Beta-driven risk (from SC factor)\n",
    "    beta_risk = factor_contributions['SC']\n",
    "    # Non-linear crisis risk (from TAIL factor)\n",
    "    tail_risk = factor_contributions['TAIL']\n",
    "    # Other factors\n",
    "    other_risk = factor_contributions['CS'] + factor_contributions['INNOV']\n",
    "    \n",
    "    print(f\"\\nContribution to downside losses:\")\n",
    "    print(f\"  Linear market beta (SC):        {beta_risk:>8.2%}  ({abs(beta_risk/total_factor_contrib)*100:>5.1f}%)\")\n",
    "    print(f\"  Non-linear crisis (TAIL):       {tail_risk:>8.2%}  ({abs(tail_risk/total_factor_contrib)*100:>5.1f}%)\")\n",
    "    print(f\"  Other factors (CS+INNOV):       {other_risk:>8.2%}  ({abs(other_risk/total_factor_contrib)*100:>5.1f}%)\")\n",
    "    print(f\"  {'─'*40}\")\n",
    "    print(f\"  Total:                     {total_factor_contrib:>8.2%}  (100.0%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CONCLUSION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"• HLPAF downside is dominated by TAIL factor ({abs(tail_risk/total_factor_contrib)*100:.0f}% of losses)\")\n",
    "    print(f\"• TAIL has low correlation with S&P ({corr_tail_spy:.2f}) → low downside beta\")\n",
    "    print(f\"• But TAIL is LARGE in downturns ({factors_down['TAIL'].mean():.1%}) → big losses\")\n",
    "    print(f\"• Result: 'Non-linear crisis exposure — you get hit during stress, but not proportionally'\")\n",
    "    print(f\"\")\n",
    "    print(f\"Why is non-linear crisis exposure WORSE than high beta risk?\")\n",
    "    print(f\"  1. Can't hedge with S&P 500 futures (TAIL correlation with S&P only {corr_tail_spy:.2f})\")\n",
    "    print(f\"  2. Crisis magnitude is unpredictable (VIX can spike 5%, 20%, or 50% for same S&P drop)\")\n",
    "    print(f\"  3. Traditional beta-based risk models (CAPM, mean-variance) miss this entirely\")\n",
    "    print(f\"  4. TAIL is systematic (market-wide stress), not diversifiable\")\n",
    "    print(f\"  5. PE has structural exposure via illiquidity, leverage, and operational stress\")\n",
    "    \n",
    "\n",
    "else:    print(\"\\nNo data available for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Decomposition: How much does each factor contribute?\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE DECOMPOSITION BY FACTOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not df_reg.empty:\n",
    "    # Get factor covariance matrix\n",
    "    factor_cov = factors_hist.cov().values\n",
    "    factor_names = factors_hist.columns.tolist()\n",
    "    \n",
    "    # Get calibrated betas as vector\n",
    "    beta_vec = betas_hl_calibrated.reindex(factor_names).values\n",
    "    \n",
    "    print(f\"\\nCalibrated betas:\")\n",
    "    for i, name in enumerate(factor_names):\n",
    "        print(f\"  {name}: {beta_vec[i]:.4f}\")\n",
    "    \n",
    "    # Total portfolio variance: β' Σ β\n",
    "    total_variance = beta_vec @ factor_cov @ beta_vec\n",
    "    total_vol = np.sqrt(total_variance)\n",
    "    \n",
    "    print(f\"\\nTotal portfolio variance: {total_variance:.6f}\")\n",
    "    print(f\"Total portfolio vol (monthly): {total_vol:.4%}\")\n",
    "    print(f\"Total portfolio vol (annual): {total_vol * np.sqrt(12):.2%}\")\n",
    "    \n",
    "    # Method 1: Marginal contribution to variance\n",
    "    # For factor i: contribution = 2 * β_i * Cov(factor_i, portfolio)\n",
    "    # Where Cov(factor_i, portfolio) = Σ_j (Σ_ij * β_j)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"METHOD 1: MARGINAL CONTRIBUTION TO VARIANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nThis measures: 'If we increase beta_i by 1%, how much does variance change?'\")\n",
    "    \n",
    "    marginal_contribs = {}\n",
    "    for i, name in enumerate(factor_names):\n",
    "        # Cov(factor_i, portfolio) = (Σ @ β)[i]\n",
    "        cov_factor_portfolio = (factor_cov @ beta_vec)[i]\n",
    "        # Marginal contribution = β_i * cov_factor_portfolio\n",
    "        marginal_contrib = beta_vec[i] * cov_factor_portfolio\n",
    "        marginal_contribs[name] = marginal_contrib\n",
    "    \n",
    "    # Marginal contributions sum to total variance\n",
    "    total_marginal = sum(marginal_contribs.values())\n",
    "    \n",
    "    print(f\"\\n{'Factor':<15} {'Beta':<10} {'Marg Contrib':<15} {'% of Variance':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for name in factor_names:\n",
    "        pct = (marginal_contribs[name] / total_variance) * 100\n",
    "        print(f\"{name:<15} {beta_vec[factor_names.index(name)]:<10.4f} {marginal_contribs[name]:<15.6f} {pct:>14.1f}%\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'TOTAL':<15} {'':<10} {total_marginal:<15.6f} {100.0:>14.1f}%\")\n",
    "    \n",
    "    # Method 2: Stand-alone contribution (ignoring correlations)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"METHOD 2: STAND-ALONE VARIANCE CONTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nThis measures: 'What if each factor moved independently (zero correlation)?'\")\n",
    "    \n",
    "    standalone_vars = {}\n",
    "    for i, name in enumerate(factor_names):\n",
    "        # Variance from factor i alone: β_i² * Var(factor_i)\n",
    "        standalone_var = (beta_vec[i]**2) * factor_cov[i, i]\n",
    "        standalone_vars[name] = standalone_var\n",
    "    \n",
    "    total_standalone = sum(standalone_vars.values())\n",
    "    \n",
    "    print(f\"\\n{'Factor':<15} {'Standalone Var':<15} {'% of Total':<15} {'Volatility':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for name in factor_names:\n",
    "        pct = (standalone_vars[name] / total_standalone) * 100\n",
    "        vol = np.sqrt(standalone_vars[name])\n",
    "        print(f\"{name:<15} {standalone_vars[name]:<15.6f} {pct:>14.1f}% {vol:>14.4%}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'SUM (uncorr.)':<15} {total_standalone:<15.6f} {100.0:>14.1f}% {np.sqrt(total_standalone):>14.4%}\")\n",
    "    print(f\"{'ACTUAL (corr.)':<15} {total_variance:<15.6f} {'':<15} {total_vol:>14.4%}\")\n",
    "    \n",
    "    # Interaction/correlation effect\n",
    "    interaction_effect = total_variance - total_standalone\n",
    "    print(f\"{'Correlation':<15} {interaction_effect:<15.6f}\")\n",
    "    print(f\"effect\")\n",
    "    \n",
    "    # Method 3: Component VaR / CVaR decomposition\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"METHOD 3: DIVERSIFICATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nSum of stand-alone volatilities: {np.sqrt(total_standalone):.2%} monthly\")\n",
    "    print(f\"Actual portfolio volatility:      {total_vol:.2%} monthly\")\n",
    "    print(f\"Diversification benefit:          {(1 - total_vol/np.sqrt(total_standalone)):.1%}\")\n",
    "    \n",
    "    # Show correlation matrix\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FACTOR CORRELATION MATRIX\")\n",
    "    print(\"=\"*80)\n",
    "    factor_corr = factors_hist.corr()\n",
    "    print(f\"\\n{'':>10}\", end='')\n",
    "    for name in factor_names:\n",
    "        print(f\"{name:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * (10 + 10*len(factor_names)))\n",
    "    for i, name1 in enumerate(factor_names):\n",
    "        print(f\"{name1:>10}\", end='')\n",
    "        for j, name2 in enumerate(factor_names):\n",
    "            print(f\"{factor_corr.iloc[i,j]:>10.3f}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"KEY INSIGHTS ON VARIANCE SOURCES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find dominant factor\n",
    "    max_factor = max(marginal_contribs.items(), key=lambda x: abs(x[1]))\n",
    "    max_pct = (marginal_contribs[max_factor[0]] / total_variance) * 100\n",
    "    \n",
    "    print(f\"\\n1. DOMINANT RISK FACTOR: {max_factor[0]}\")\n",
    "    print(f\"   Contributes {max_pct:.1f}% of total variance\")\n",
    "    print(f\"   Beta: {beta_vec[factor_names.index(max_factor[0])]:.4f}\")\n",
    "    \n",
    "    # Check for negative contributors (hedging effect)\n",
    "    negative_contribs = {k: v for k, v in marginal_contribs.items() if v < 0}\n",
    "    if negative_contribs:\n",
    "        print(f\"\\n2. NATURAL HEDGES (negative variance contribution):\")\n",
    "        for name, contrib in negative_contribs.items():\n",
    "            pct = (contrib / total_variance) * 100\n",
    "            print(f\"   {name}: {pct:.1f}% (reduces variance)\")\n",
    "    \n",
    "    # Diversification from correlations\n",
    "    if interaction_effect < 0:\n",
    "        print(f\"\\n3. DIVERSIFICATION BENEFIT:\")\n",
    "        print(f\"   Correlations reduce variance by {abs(interaction_effect/total_standalone)*100:.1f}%\")\n",
    "        print(f\"   Without correlations, vol would be {np.sqrt(total_standalone):.2%}\")\n",
    "        print(f\"   With correlations, vol is {total_vol:.2%}\")\n",
    "    else:\n",
    "        print(f\"\\n3. CONCENTRATION RISK:\")\n",
    "        print(f\"   Positive correlations INCREASE variance by {abs(interaction_effect/total_standalone)*100:.1f}%\")\n",
    "        print(f\"   Without correlations, vol would be {np.sqrt(total_standalone):.2%}\")\n",
    "        print(f\"   With correlations, vol is {total_vol:.2%}\")\n",
    "    \n",
    "    # Rank factors by contribution\n",
    "    print(f\"\\n4. FACTOR RANKING (by variance contribution):\")\n",
    "    sorted_factors = sorted(marginal_contribs.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    for rank, (name, contrib) in enumerate(sorted_factors, 1):\n",
    "        pct = (contrib / total_variance) * 100\n",
    "        print(f\"   {rank}. {name:<10} {pct:>6.1f}%\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nNo data available for variance decomposition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fec8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clarify the volatility metrics and how they're calculated\n",
    "print(\"=\"*60)\n",
    "print(\"VOLATILITY METRICS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Annualized volatility of monthly returns (the traditional measure)\n",
    "ann_vol_hl = r_hl.std() * np.sqrt(12)\n",
    "ann_vol_sp = r_sp.std() * np.sqrt(12)\n",
    "\n",
    "print(f\"\\n1. Annualized volatility of monthly returns:\")\n",
    "print(f\"   How calculated: std(monthly returns) × √12\")\n",
    "print(f\"   HLPAF:   {ann_vol_hl:.2%}\")\n",
    "print(f\"   S&P 500: {ann_vol_sp:.2%}\")\n",
    "\n",
    "# Standard deviation of annual returns (directly from Monte Carlo)\n",
    "print(f\"\\n2. Std dev of annual returns (from Monte Carlo):\")\n",
    "print(f\"   How calculated:\")\n",
    "print(f\"     - Simulate {n_paths:,} paths of {n_steps} monthly returns\")\n",
    "print(f\"     - For each path: compound 12 months → annual return\")\n",
    "print(f\"     - Take std dev across the {n_paths:,} annual returns\")\n",
    "print(f\"   HLPAF:   {ann_hl.std():.2%}\")\n",
    "print(f\"   S&P 500: {ann_sp.std():.2%}\")\n",
    "\n",
    "print(f\"\\n3. Why they're different:\")\n",
    "print(f\"   Metric #1: Volatility of monthly returns (scaled to annual)\")\n",
    "print(f\"   Metric #2: Actual volatility of annual returns (from simulations)\")\n",
    "print(f\"   They should be very close for random walk processes.\")\n",
    "print(f\"   Small differences arise from compounding effects and sampling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08505297",
   "metadata": {},
   "source": [
    "## Export Notebook to Markdown\n",
    "\n",
    "Generate a comprehensive markdown report with all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9122c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.export_from_notebook import export_notebook_to_markdown\n",
    "\n",
    "# Run the export\n",
    "notebook_path = '07_HLPAF_Analysis.ipynb'\n",
    "output_path = '../artifacts/HLPAF_Analysis_Report.md'\n",
    "\n",
    "try:\n",
    "    export_notebook_to_markdown(notebook_path, output_path,\n",
    "                                show_all=False,\n",
    "                                show_markdown=True,                               \n",
    "                                show_html = False,\n",
    "                                show_code=False, \n",
    "                                show_cell_output=True\n",
    "                                )\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting notebook: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
